## 🔍 什么是Transformer？

Transformer 是由 Vaswani 等人在 2017 年提出的模型架构，用于处理序列到序列的任务（如机器翻译）。它使用 **自注意力机制（Self-Attention）** 代替传统的 RNN/CNN 架构，是 GPT 和 BERT 等预训练语言模型的基础。

> 论文出处：*Attention is All You Need*

---

## Transformer 的主要模块

### 1. 输入嵌入（Input Embedding）

- **作用**：输入文本（通常是一个词或子词）被转化为稠密的向量表示。每个词通过查找词汇表得到一个嵌入向量。
- **位置编码（Positional Encoding）**：
  - 由于 Transformer 没有递归或卷积的结构，它无法自然地理解序列中词汇的位置信息。因此，**位置编码**用于为每个词添加位置信息。
  - 位置编码通常使用正弦和余弦函数进行计算，确保每个位置有唯一的向量表示。通过这种方式，模型能够感知到序列中各词的相对或绝对位置。
  
- **公式**：
    $$ PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) $$  
    $$ PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) $$  
    其中，`pos` 是词的位置，`i` 是词向量的维度，`d_model` 是模型的总维度。

### 2. 多头自注意力机制（Multi-Head Self-Attention）

- **作用**：自注意力机制使得模型能够同时关注输入序列中不同位置的词。它通过计算每个单词与所有其他单词的关系来生成表示，从而捕捉序列中的长距离依赖关系。
- **工作原理**：
  - **查询（Query）、键（Key）、值（Value）**：自注意力机制的核心是通过查询、键和值三个向量来计算词之间的相关性。每个输入单元（词）都会生成这三个向量。
  - **注意力计算**：通过计算查询和键的点积来衡量它们的相似性。接着，使用 softmax 函数将相似度转化为概率分布，并利用该概率对值向量进行加权求和。
  
- **多头注意力**：
  - 传统的单头注意力机制只从一个“子空间”捕捉信息。多头注意力机制通过并行计算多个注意力头，每个头能够在不同的子空间中学习信息。然后，将这些头的输出进行拼接，最后通过线性变换进行整合。
  
- **公式**：
    $$ \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)V $$  
    其中，`Q` 是查询向量，`K` 是键向量，`V` 是值向量，`d_k` 是键向量的维度。

### 3. 前馈神经网络（Feed-Forward Network）

- **作用**：在每个自注意力模块之后，Transformer 使用一个前馈神经网络进行进一步的特征转换。该网络通常由两个全连接层组成，之间使用非线性激活函数（如 ReLU）来引入非线性能力。
- **结构**：
  - **第一个全连接层**：将输入的向量维度扩展到更大的维度。
  - **激活函数**：常用 ReLU 激活函数，使模型具有更强的表达能力。
  - **第二个全连接层**：将向量映射回原始的嵌入维度。

- **公式**：
    $$ FF(x) = \text{max}(0, xW_1 + b_1)W_2 + b_2 $$  
    其中，`W_1` 和 `W_2` 是权重矩阵，`b_1` 和 `b_2` 是偏置项。

### 4. 残差连接和 LayerNorm（Layer Normalization）

- **作用**：为了缓解深层神经网络中可能出现的梯度消失或梯度爆炸问题，Transformer 在每个子层之后使用了 **残差连接**（Residual Connection）。通过这种方式，输入信息能够在网络中直接传播，从而使训练变得更加稳定。
- **残差连接**：
  - 每个子层的输入会与该子层的输出相加，然后再经过标准化处理。这种结构有助于加速训练并提高模型的表现。
  
- **Layer Normalization**：
  - Transformer 中使用 **LayerNorm** 对每个子层的输出进行归一化。这有助于减少不同特征间的分布差异，从而加速模型的训练过程。
  - LayerNorm 通常应用于输入的每一层，以确保每个层的输出具有零均值和单位方差，使得每层的特征分布更为一致。

- **优点**：
  - **残差连接**：帮助缓解梯度消失问题，使得信息能够在深层网络中直接流动。
  - **LayerNorm**：提高训练稳定性，尤其是在深层网络中，防止了梯度爆炸和梯度消失问题。

### 5. 输出层（Output Layer）

- **作用**：Transformer 模型的最后一层通常是一个 **全连接层**，将高维的特征向量映射到输出空间，通常是用于分类或回归任务的类别数或标签数。
- **工作原理**：
  - 输出层将通过注意力机制和前馈神经网络处理后的信息映射到目标任务的输出空间，例如在文本分类任务中，输出层的输出维度通常与类别数相同。

- **Softmax**：
  - 对于分类任务，输出通常通过 **Softmax** 函数进行归一化，得到一个概率分布，这样模型就可以输出最有可能的类别。

---
## 📚 参考资料

- **Transformer、GPT、BERT，预训练语言模型的前世今生**
- **基于transformers的自然语言处理(NLP)入门**


